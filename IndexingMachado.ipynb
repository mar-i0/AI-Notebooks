{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOSHf+A90qU8yiVOPmp+7IM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mar-i0/AI-Notebooks/blob/main/IndexingMachado.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "!pip install langchain\n",
        "!pip install llama_index\n",
        "!pip install git+https://github.com/huggingface/transformers.git accelerate\n",
        "!pip install typing\n",
        "!pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpzZRQiOHihV",
        "outputId": "e9946b53-8c5c-4112-bdd3-86308c3f7906"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (2.0.0+cu118)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch) (3.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch) (16.0.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.9/dist-packages (0.0.142)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.9/dist-packages (from langchain) (6.0)\n",
            "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /usr/local/lib/python3.9/dist-packages (from langchain) (1.2.4)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain) (1.22.4)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.9/dist-packages (from langchain) (2.8.4)\n",
            "Requirement already satisfied: gptcache>=0.1.7 in /usr/local/lib/python3.9/dist-packages (from langchain) (0.1.16)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.9/dist-packages (from langchain) (2.27.1)\n",
            "Requirement already satisfied: SQLAlchemy<2,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain) (1.4.47)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.9/dist-packages (from langchain) (3.8.4)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain) (1.10.7)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.9/dist-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.9/dist-packages (from langchain) (0.5.7)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from langchain) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.2)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
            "Requirement already satisfied: typing-inspect>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.8.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /usr/local/lib/python3.9/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.19.0)\n",
            "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /usr/local/lib/python3.9/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (1.5.1)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.9/dist-packages (from gptcache>=0.1.7->langchain) (5.3.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.9/dist-packages (from gptcache>=0.1.7->langchain) (0.27.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic<2,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2->langchain) (1.26.15)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.9/dist-packages (from SQLAlchemy<2,>=1->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.9/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai->gptcache>=0.1.7->langchain) (4.65.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: llama_index in /usr/local/lib/python3.9/dist-packages (0.5.18)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.9/dist-packages (from llama_index) (8.2.2)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.9/dist-packages (from llama_index) (0.5.7)\n",
            "Requirement already satisfied: langchain==0.0.142 in /usr/local/lib/python3.9/dist-packages (from llama_index) (0.0.142)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from llama_index) (1.5.3)\n",
            "Requirement already satisfied: openai>=0.26.4 in /usr/local/lib/python3.9/dist-packages (from llama_index) (0.27.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from llama_index) (1.22.4)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.9/dist-packages (from llama_index) (0.3.3)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.9/dist-packages (from langchain==0.0.142->llama_index) (2.8.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.9/dist-packages (from langchain==0.0.142->llama_index) (2.27.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from langchain==0.0.142->llama_index) (4.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.9/dist-packages (from langchain==0.0.142->llama_index) (3.8.4)\n",
            "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /usr/local/lib/python3.9/dist-packages (from langchain==0.0.142->llama_index) (1.2.4)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain==0.0.142->llama_index) (1.10.7)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.9/dist-packages (from langchain==0.0.142->llama_index) (6.0)\n",
            "Requirement already satisfied: gptcache>=0.1.7 in /usr/local/lib/python3.9/dist-packages (from langchain==0.0.142->llama_index) (0.1.16)\n",
            "Requirement already satisfied: SQLAlchemy<2,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain==0.0.142->llama_index) (1.4.47)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /usr/local/lib/python3.9/dist-packages (from dataclasses-json->llama_index) (3.19.0)\n",
            "Requirement already satisfied: typing-inspect>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from dataclasses-json->llama_index) (0.8.0)\n",
            "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /usr/local/lib/python3.9/dist-packages (from dataclasses-json->llama_index) (1.5.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai>=0.26.4->llama_index) (4.65.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->llama_index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->llama_index) (2022.7.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.9/dist-packages (from tiktoken->llama_index) (2022.10.31)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.142->llama_index) (1.8.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.142->llama_index) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.142->llama_index) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.142->llama_index) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.142->llama_index) (6.0.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.142->llama_index) (1.3.3)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.9/dist-packages (from gptcache>=0.1.7->langchain==0.0.142->llama_index) (5.3.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.9/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json->llama_index) (23.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic<2,>=1->langchain==0.0.142->llama_index) (4.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->llama_index) (1.16.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2->langchain==0.0.142->llama_index) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2->langchain==0.0.142->llama_index) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2->langchain==0.0.142->llama_index) (2022.12.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.9/dist-packages (from SQLAlchemy<2,>=1->langchain==0.0.142->llama_index) (2.0.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from typing-inspect>=0.4.0->dataclasses-json->llama_index) (1.0.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/huggingface/transformers.git\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-vjzfki0w\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-vjzfki0w\n",
            "  Resolved https://github.com/huggingface/transformers.git to commit 474bf508dfe0d46fc38585a1bb793e5ba74fddfd\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.9/dist-packages (0.18.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.29.0.dev0) (0.13.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.29.0.dev0) (2022.10.31)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.29.0.dev0) (23.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers==4.29.0.dev0) (3.11.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.29.0.dev0) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.29.0.dev0) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers==4.29.0.dev0) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers==4.29.0.dev0) (4.65.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.29.0.dev0) (0.13.4)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from accelerate) (2.0.0+cu118)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.29.0.dev0) (4.5.0)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate) (3.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate) (1.11.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.4.0->accelerate) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.4.0->accelerate) (16.0.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.29.0.dev0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.29.0.dev0) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.29.0.dev0) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.29.0.dev0) (2.0.12)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.4.0->accelerate) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.4.0->accelerate) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.9/dist-packages (3.7.4.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.9/dist-packages (3.0.1)\n",
            "Requirement already satisfied: typing_extensions>=3.10.0.0 in /usr/local/lib/python3.9/dist-packages (from PyPDF2) (4.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tW_nTHlpHTGd",
        "outputId": "d77f29fb-0d0b-4b6a-a761-73c8e67b88db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;30;43mSe han truncado las últimas 5000 líneas del flujo de salida.\u001b[0m\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2020) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 48, in <cell line: 48>\n",
            "    response = index.query(\"Almas dichosas que del mortal velo\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=1580) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 48, in <cell line: 48>\n",
            "    response = index.query(\"Almas dichosas que del mortal velo\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2043) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 48, in <cell line: 48>\n",
            "    response = index.query(\"Almas dichosas que del mortal velo\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2027) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 48, in <cell line: 48>\n",
            "    response = index.query(\"Almas dichosas que del mortal velo\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=1577) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 48, in <cell line: 48>\n",
            "    response = index.query(\"Almas dichosas que del mortal velo\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 48, in <cell line: 48>\n",
            "    response = index.query(\"Almas dichosas que del mortal velo\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2014) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 48, in <cell line: 48>\n",
            "    response = index.query(\"Almas dichosas que del mortal velo\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=1575) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 48, in <cell line: 48>\n",
            "    response = index.query(\"Almas dichosas que del mortal velo\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2017) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 48, in <cell line: 48>\n",
            "    response = index.query(\"Almas dichosas que del mortal velo\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2040) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 48, in <cell line: 48>\n",
            "    response = index.query(\"Almas dichosas que del mortal velo\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=1604) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 48, in <cell line: 48>\n",
            "    response = index.query(\"Almas dichosas que del mortal velo\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2040) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 48, in <cell line: 48>\n",
            "    response = index.query(\"Almas dichosas que del mortal velo\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2027) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 48, in <cell line: 48>\n",
            "    response = index.query(\"Almas dichosas que del mortal velo\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=1573) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 48, in <cell line: 48>\n",
            "    response = index.query(\"Almas dichosas que del mortal velo\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2027) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 48, in <cell line: 48>\n",
            "    response = index.query(\"Almas dichosas que del mortal velo\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2025) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 48, in <cell line: 48>\n",
            "    response = index.query(\"Almas dichosas que del mortal velo\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=1590) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 48, in <cell line: 48>\n",
            "    response = index.query(\"Almas dichosas que del mortal velo\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2021) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 48, in <cell line: 48>\n",
            "    response = index.query(\"Almas dichosas que del mortal velo\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2033) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 48, in <cell line: 48>\n",
            "    response = index.query(\"Almas dichosas que del mortal velo\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=1572) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 48, in <cell line: 48>\n",
            "    response = index.query(\"Almas dichosas que del mortal velo\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2012) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 48, in <cell line: 48>\n",
            "    response = index.query(\"Almas dichosas que del mortal velo\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2046) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 48, in <cell line: 48>\n",
            "    response = index.query(\"Almas dichosas que del mortal velo\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=1608) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 48, in <cell line: 48>\n",
            "    response = index.query(\"Almas dichosas que del mortal velo\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 48, in <cell line: 48>\n",
            "    response = index.query(\"Almas dichosas que del mortal velo\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2039) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 48, in <cell line: 48>\n",
            "    response = index.query(\"Almas dichosas que del mortal velo\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=1596) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 48, in <cell line: 48>\n",
            "    response = index.query(\"Almas dichosas que del mortal velo\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2029) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 48, in <cell line: 48>\n",
            "    response = index.query(\"Almas dichosas que del mortal velo\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2046) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 48, in <cell line: 48>\n",
            "    response = index.query(\"Almas dichosas que del mortal velo\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=1588) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 48, in <cell line: 48>\n",
            "    response = index.query(\"Almas dichosas que del mortal velo\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2028) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 48, in <cell line: 48>\n",
            "    response = index.query(\"Almas dichosas que del mortal velo\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2027) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 48, in <cell line: 48>\n",
            "    response = index.query(\"Almas dichosas que del mortal velo\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=1598) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 48, in <cell line: 48>\n",
            "    response = index.query(\"Almas dichosas que del mortal velo\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2017) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 48, in <cell line: 48>\n",
            "    response = index.query(\"Almas dichosas que del mortal velo\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2040) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 48, in <cell line: 48>\n",
            "    response = index.query(\"Almas dichosas que del mortal velo\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=1592) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 48, in <cell line: 48>\n",
            "    response = index.query(\"Almas dichosas que del mortal velo\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2006) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 48, in <cell line: 48>\n",
            "    response = index.query(\"Almas dichosas que del mortal velo\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=979) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "------------\n",
            "QUERES QUIJOTE, Y QUIJOTE QUIJOTE QUIJOTE QUIJOTE QUIJOTE QUIJOTE QUIJOTE QUIJOTE QUIJOTE QUIJOTE QUIJOTE QUIJOTE QUIJOTE QUIJOTE QUIJOTE QUIJOTE QUIJOTE QUIJOTE QUIJOTE QUIJOTE QUIJOTE QUIJOTE QUIJOTE QUIJOTE QUIJOTE QUIJOTE QUIJOTE QUIJOTE QUIJOTE QUIJOTE QUIJOTE QUIJOTE QUIJOTE QUIJOTE QUIJOTE QUIJOTE QUIJOTE QUIJOTE QUIJOTE QUIJOTE QUIJOTE QUIJOTE QUIJOTE QUIJOTE QUIJOTE QUIJOTE QUIJOTE QUIJOTE QUIJOTE QUIJOTE QUIJOTE QUIJOTE QUIJOTE QUIJOTE QUIJOTE QUIJOTE QUIJOTE QUIJOTE QUIJOTE QUIJOTE QUIJOTE\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch\n",
        "from langchain.llms.base import LLM\n",
        "from llama_index import SimpleDirectoryReader, LangchainEmbedding, GPTListIndex, PromptHelper\n",
        "from llama_index import LLMPredictor, ServiceContext\n",
        "from transformers import pipeline\n",
        "from typing import Optional, List, Mapping, Any\n",
        "\n",
        "\n",
        "# define prompt helper\n",
        "# set maximum input size\n",
        "max_input_size = 2048\n",
        "# set number of output tokens\n",
        "num_output = 256\n",
        "# set maximum chunk overlap\n",
        "max_chunk_overlap = 20\n",
        "prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n",
        "\n",
        "\n",
        "class CustomLLM(LLM):\n",
        "    model_name = \"facebook/opt-1.3b\"\n",
        "    pipeline = pipeline(\"text-generation\", model=model_name, device=\"cuda:0\", model_kwargs={\"torch_dtype\":torch.bfloat16})\n",
        "\n",
        "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
        "        prompt_length = len(prompt)\n",
        "        response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
        "\n",
        "        # only return newly generated tokens\n",
        "        return response[prompt_length:]\n",
        "\n",
        "    @property\n",
        "    def _identifying_params(self) -> Mapping[str, Any]:\n",
        "        return {\"name_of_model\": self.model_name}\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"custom\"\n",
        "\n",
        "# define our LLM\n",
        "llm_predictor = LLMPredictor(llm=CustomLLM())\n",
        "\n",
        "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n",
        "\n",
        "# Load the your data\n",
        "documents = SimpleDirectoryReader('./data').load_data()\n",
        "index = GPTListIndex.from_documents(documents, service_context=service_context)\n",
        "\n",
        "# Query and print response\n",
        "response = index.query(\"Almas dichosas que del mortal velo\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = index.query(\"¿De qué color es el caballo blanco de Santiago?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5MI5ksBOqFl",
        "outputId": "661e4717-7778-4904-def5-5197d26d0c87"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;30;43mSe han truncado las últimas 5000 líneas del flujo de salida.\u001b[0m\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=1593) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-3-b2771b88c00f>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"¿De qué color es el caballo blanco de Santiago?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2025) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-3-b2771b88c00f>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"¿De qué color es el caballo blanco de Santiago?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2018) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-3-b2771b88c00f>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"¿De qué color es el caballo blanco de Santiago?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=1596) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-3-b2771b88c00f>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"¿De qué color es el caballo blanco de Santiago?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2044) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-3-b2771b88c00f>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"¿De qué color es el caballo blanco de Santiago?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2028) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-3-b2771b88c00f>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"¿De qué color es el caballo blanco de Santiago?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=1591) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-3-b2771b88c00f>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"¿De qué color es el caballo blanco de Santiago?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2026) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-3-b2771b88c00f>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"¿De qué color es el caballo blanco de Santiago?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2010) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-3-b2771b88c00f>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"¿De qué color es el caballo blanco de Santiago?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=1597) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-3-b2771b88c00f>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"¿De qué color es el caballo blanco de Santiago?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2014) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-3-b2771b88c00f>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"¿De qué color es el caballo blanco de Santiago?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2041) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-3-b2771b88c00f>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"¿De qué color es el caballo blanco de Santiago?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=1623) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-3-b2771b88c00f>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"¿De qué color es el caballo blanco de Santiago?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2037) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-3-b2771b88c00f>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"¿De qué color es el caballo blanco de Santiago?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2027) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-3-b2771b88c00f>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"¿De qué color es el caballo blanco de Santiago?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=1591) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-3-b2771b88c00f>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"¿De qué color es el caballo blanco de Santiago?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2027) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-3-b2771b88c00f>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"¿De qué color es el caballo blanco de Santiago?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2025) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-3-b2771b88c00f>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"¿De qué color es el caballo blanco de Santiago?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=1607) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-3-b2771b88c00f>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"¿De qué color es el caballo blanco de Santiago?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2022) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-3-b2771b88c00f>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"¿De qué color es el caballo blanco de Santiago?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2034) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-3-b2771b88c00f>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"¿De qué color es el caballo blanco de Santiago?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=1594) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-3-b2771b88c00f>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"¿De qué color es el caballo blanco de Santiago?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2009) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-3-b2771b88c00f>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"¿De qué color es el caballo blanco de Santiago?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-3-b2771b88c00f>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"¿De qué color es el caballo blanco de Santiago?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=1623) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-3-b2771b88c00f>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"¿De qué color es el caballo blanco de Santiago?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-3-b2771b88c00f>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"¿De qué color es el caballo blanco de Santiago?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2040) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-3-b2771b88c00f>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"¿De qué color es el caballo blanco de Santiago?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=1611) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-3-b2771b88c00f>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"¿De qué color es el caballo blanco de Santiago?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2029) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-3-b2771b88c00f>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"¿De qué color es el caballo blanco de Santiago?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2044) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-3-b2771b88c00f>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"¿De qué color es el caballo blanco de Santiago?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=1600) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-3-b2771b88c00f>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"¿De qué color es el caballo blanco de Santiago?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2030) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-3-b2771b88c00f>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"¿De qué color es el caballo blanco de Santiago?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2029) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-3-b2771b88c00f>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"¿De qué color es el caballo blanco de Santiago?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=1613) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-3-b2771b88c00f>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"¿De qué color es el caballo blanco de Santiago?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2015) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-3-b2771b88c00f>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"¿De qué color es el caballo blanco de Santiago?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2039) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-3-b2771b88c00f>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"¿De qué color es el caballo blanco de Santiago?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=1610) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-3-b2771b88c00f>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"¿De qué color es el caballo blanco de Santiago?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2011) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-3-b2771b88c00f>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"¿De qué color es el caballo blanco de Santiago?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=984) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "------------\n",
            "\n",
            "The original question is as follows: ¿De qué color es el caballo blanco de Santiago?\n",
            "We have provided an existing answer:\n",
            "------------\n",
            "y el fin al principio y al medio\n",
            "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
            "------------\n",
            "porque si al cabo de haber andado caminos y carreras, y pasado malas noches y peores días, ha de venir a coger el fruto de nuestros trabajos el que se está holgando en esta venta, no hay para qué darme priesa a qué ensille a Rocinante, albarde el jumento y aderece el palafrén, pues sera mejor\n",
            "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
            "------------\n",
            "contó punto por punto la volatería de Sancho Panza\n",
            "We have the opportunity to refine the existing answer\n",
            "We have the opportunity to refine the existing answer (only if\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = index.query(\"Caminante ¿Que es el camino y nada más?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIQY6hdVXTcF",
        "outputId": "1ef87c81-1764-4f1d-f656-a1c6d569cd05"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;30;43mSe han truncado las últimas 5000 líneas del flujo de salida.\u001b[0m\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2019) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-4-fd1e80cf50ce>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"Caminante ¿Que es el camino y nada más?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=1597) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-4-fd1e80cf50ce>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"Caminante ¿Que es el camino y nada más?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2044) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-4-fd1e80cf50ce>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"Caminante ¿Que es el camino y nada más?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2027) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-4-fd1e80cf50ce>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"Caminante ¿Que es el camino y nada más?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=1598) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-4-fd1e80cf50ce>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"Caminante ¿Que es el camino y nada más?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2027) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-4-fd1e80cf50ce>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"Caminante ¿Que es el camino y nada más?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2011) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-4-fd1e80cf50ce>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"Caminante ¿Que es el camino y nada más?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=1598) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-4-fd1e80cf50ce>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"Caminante ¿Que es el camino y nada más?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2015) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-4-fd1e80cf50ce>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"Caminante ¿Que es el camino y nada más?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2042) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-4-fd1e80cf50ce>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"Caminante ¿Que es el camino y nada más?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=1624) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-4-fd1e80cf50ce>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"Caminante ¿Que es el camino y nada más?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2038) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-4-fd1e80cf50ce>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"Caminante ¿Que es el camino y nada más?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2026) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-4-fd1e80cf50ce>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"Caminante ¿Que es el camino y nada más?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=1592) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-4-fd1e80cf50ce>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"Caminante ¿Que es el camino y nada más?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2027) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-4-fd1e80cf50ce>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"Caminante ¿Que es el camino y nada más?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2026) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-4-fd1e80cf50ce>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"Caminante ¿Que es el camino y nada más?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=1608) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-4-fd1e80cf50ce>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"Caminante ¿Que es el camino y nada más?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2022) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-4-fd1e80cf50ce>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"Caminante ¿Que es el camino y nada más?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2035) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-4-fd1e80cf50ce>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"Caminante ¿Que es el camino y nada más?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=1595) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-4-fd1e80cf50ce>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"Caminante ¿Que es el camino y nada más?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2010) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-4-fd1e80cf50ce>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"Caminante ¿Que es el camino y nada más?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-4-fd1e80cf50ce>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"Caminante ¿Que es el camino y nada más?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=1627) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-4-fd1e80cf50ce>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"Caminante ¿Que es el camino y nada más?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-4-fd1e80cf50ce>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"Caminante ¿Que es el camino y nada más?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2040) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-4-fd1e80cf50ce>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"Caminante ¿Que es el camino y nada más?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=1612) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-4-fd1e80cf50ce>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"Caminante ¿Que es el camino y nada más?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2029) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-4-fd1e80cf50ce>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"Caminante ¿Que es el camino y nada más?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2046) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-4-fd1e80cf50ce>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"Caminante ¿Que es el camino y nada más?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=1607) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-4-fd1e80cf50ce>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"Caminante ¿Que es el camino y nada más?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2028) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-4-fd1e80cf50ce>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"Caminante ¿Que es el camino y nada más?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2029) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-4-fd1e80cf50ce>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"Caminante ¿Que es el camino y nada más?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=1616) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-4-fd1e80cf50ce>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"Caminante ¿Que es el camino y nada más?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2016) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-4-fd1e80cf50ce>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"Caminante ¿Que es el camino y nada más?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2040) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-4-fd1e80cf50ce>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"Caminante ¿Que es el camino y nada más?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=1611) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-4-fd1e80cf50ce>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"Caminante ¿Que es el camino y nada más?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=2012) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-4-fd1e80cf50ce>\", line 1, in <cell line: 1>\n",
            "    response = index.query(\"Caminante ¿Que es el camino y nada más?\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py\", line 260, in query\n",
            "    return query_runner.query(query_str)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 349, in query\n",
            "    return query_combiner.run(query_bundle, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_combiner/base.py\", line 68, in run\n",
            "    return self._query_runner.query_transformed(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/query_runner.py\", line 209, in query_transformed\n",
            "    return query_obj.query(query_bundle)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n",
            "    f_return_val = f(_self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 207, in query\n",
            "    return self.synthesize(query_bundle, nodes)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\", line 184, in synthesize\n",
            "    return self._response_synthesizer.synthesize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_synthesis.py\", line 123, in synthesize\n",
            "    response_str = self._response_builder.get_response(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 130, in get_response\n",
            "    response = self._refine_response_single(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\", line 217, in _refine_response_single\n",
            "    ) = self._service_context.llm_predictor.predict(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 223, in predict\n",
            "    llm_prediction = self._predict(prompt, **prompt_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 197, in _predict\n",
            "    llm_prediction = retry_on_exceptions_with_backoff(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
            "    return lambda_fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\", line 198, in <lambda>\n",
            "    lambda: llm_chain.predict(**full_prompt_args),\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 151, in predict\n",
            "    return self(kwargs)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\", line 113, in __call__\n",
            "    outputs = self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 57, in _call\n",
            "    return self.apply([inputs])[0]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 118, in apply\n",
            "    response = self.generate(input_list)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\", line 62, in generate\n",
            "    return self.llm.generate_prompt(prompts, stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 137, in generate\n",
            "    output = self._generate(prompts, stop=stop)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\", line 324, in _generate\n",
            "    text = self._call(prompt, stop=stop)\n",
            "  File \"<ipython-input-2-7ca191b2f178>\", line 25, in _call\n",
            "    response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 209, in __call__\n",
            "    return super().__call__(text_inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1109, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1116, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\", line 1015, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_generation.py\", line 251, in _forward\n",
            "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1350, in generate\n",
            "    logger.warn(\n",
            "Message: 'Both `max_new_tokens` (=256) and `max_length`(=985) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
            "Arguments: (<class 'UserWarning'>,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "------------\n",
            "Pago lo que se debía. El canónigo pidió al cura le avisase el suceso de don Quijote, si sanaba de su locura, o si proseguía en ella, y con esto, tomó licencia para seguir su viaje. En fin, todos se dividieron y apartaron, quedando solos el cura y barbero, don Quijote y Panza y el bueno de Rocinante, que a todo lo que había visto estaba con tanta paciencia como su amo.\n",
            "El boyero unció sus bueyes y acomodó a don Quijote sobre un haz de heno, y con su acostumbrada flema siguió el camino que el cura quiso, y a cabo de seis días llegaron a la aldea de don Quijote, adonde entraron en la mitad del día, que acertó a ser domingo,\n",
            "\n"
          ]
        }
      ]
    }
  ]
}